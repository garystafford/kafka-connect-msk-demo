{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Demonstration: Apache Hudi with PySpark, Kafka, Hive, and S3\n",
    "\n",
    "__Purpose:__ Read messages from Kafka topic in JSON format and write to Amazon S3 as Parquet using Apache Hudi: Upserts and Delete  \n",
    "__Author:__  Gary A. Stafford  \n",
    "__Date:__ 2021-10-03  \n",
    "__References:__  \n",
    "- https://hudi.apache.org/docs/quick-start-guide/\n",
    "- https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html\n",
    "- https://hudi.apache.org/docs/configurations#SPARK_DATASOURCE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run commands from master node\n",
    "\n",
    "SSH to EMR master node as `hadoop` user.\n",
    "\n",
    "```shell\n",
    "hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/jars/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pagila-sales-hudi\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\":\n",
    "            \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "        \"spark.serializer\":\n",
    "            \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\":\n",
    "            \"false\"\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc.getConf().getAll()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import pyspark.sql.functions as F\n",
    "from ec2_metadata import ec2_metadata\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType, TimestampType"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_parameters():\n",
    "    \"\"\"Load parameter values from AWS Systems Manager (SSM) Parameter Store\"\"\"\n",
    "\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = ec2_metadata.region\n",
    "    ssm_client = boto3.client(\"ssm\")\n",
    "\n",
    "    parameters = {\n",
    "        \"kafka_servers\":\n",
    "            ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_servers\")\n",
    "            [\"Parameter\"][\"Value\"],\n",
    "        \"kafka_demo_bucket\":\n",
    "            ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_demo_bucket\")\n",
    "            [\"Parameter\"][\"Value\"],\n",
    "        \"schema_registry_url\":\n",
    "            ssm_client.get_parameter(\n",
    "                Name=\"/kafka_spark_demo/schema_registry_url_int\")[\"Parameter\"]\n",
    "            [\"Value\"],\n",
    "    }\n",
    "\n",
    "    return parameters"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_from_kafka(topic, schema):\n",
    "    \"\"\"Batch read messages from Kafka topic, convert from binary and deserialize JSON\"\"\"\n",
    "\n",
    "    options_read_kafka = {\n",
    "        \"kafka.bootstrap.servers\":\n",
    "            params[\"kafka_servers\"],\n",
    "        \"subscribe\":\n",
    "            kafka_topic,\n",
    "        \"startingOffsets\":\n",
    "            \"earliest\",\n",
    "        \"endingOffsets\":\n",
    "            \"latest\",\n",
    "        \"kafka.ssl.truststore.location\":\n",
    "            \"/tmp/kafka.client.truststore.jks\",\n",
    "        \"kafka.security.protocol\":\n",
    "            \"SASL_SSL\",\n",
    "        \"kafka.sasl.mechanism\":\n",
    "            \"AWS_MSK_IAM\",\n",
    "        \"kafka.sasl.jaas.config\":\n",
    "            \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "        \"kafka.sasl.client.callback.handler.class\":\n",
    "            \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\"\n",
    "    }\n",
    "\n",
    "    df = spark.read \\\n",
    "        .format(\"kafka\") \\\n",
    "        .options(**options_read_kafka) \\\n",
    "        .load() \\\n",
    "        .selectExpr(\"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "        .select(F.from_json(\"value\", schema=schema).alias(\"data\"), \"timestamp\") \\\n",
    "        .select(\"data.*\", \"timestamp\")\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def write_to_kafka(df):\n",
    "    \"\"\"Serialize JSON-format messages and batch write to Kafka topic\"\"\"\n",
    "\n",
    "    options_write = {\n",
    "        \"kafka.bootstrap.servers\":\n",
    "            params[\"kafka_servers\"],\n",
    "        \"topic\":\n",
    "            kafka_topic,\n",
    "        \"kafka.ssl.truststore.location\":\n",
    "            \"/tmp/kafka.client.truststore.jks\",\n",
    "        \"kafka.security.protocol\":\n",
    "            \"SASL_SSL\",\n",
    "        \"kafka.sasl.mechanism\":\n",
    "            \"AWS_MSK_IAM\",\n",
    "        \"kafka.sasl.jaas.config\":\n",
    "            \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "        \"kafka.sasl.client.callback.handler.class\":\n",
    "            \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\",\n",
    "    }\n",
    "\n",
    "    df \\\n",
    "        .selectExpr(\"CAST(payment_id AS STRING) AS key\",\n",
    "                    \"to_json(struct(*)) AS value\") \\\n",
    "        .write \\\n",
    "        .format(\"kafka\") \\\n",
    "        .options(**options_write) \\\n",
    "        .save()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = get_parameters()\n",
    "\n",
    "kafka_topic = \"pagila.sales.spark.streaming\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"amount\", FloatType(), False),\n",
    "    StructField(\"payment_date\", TimestampType(), False),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"district\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# batch read all messages from kafka topic\n",
    "\n",
    "df_sales = read_from_kafka(kafka_topic, schema)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%display -n 5\n",
    "df_sales"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# set hudi write config\n",
    "\n",
    "table_name = \"hudi.hudi_pagila_sales\"\n",
    "base_path = f\"s3://{params['kafka_demo_bucket']}/hudi/\"\n",
    "\n",
    "options_write_hudi = {\n",
    "    \"hoodie.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"payment_id\",\n",
    "    \"hoodie.datasource.write.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"country\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"timestamp\",\n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.datasource.hive_sync.enable\": True,\n",
    "    \"hoodie.datasource.hive_sync.assume_date_partitioning\": False,\n",
    "    \"hoodie.datasource.hive_sync.database\": \"hudi\",\n",
    "    \"hoodie.datasource.hive_sync.auto_create_database\": True,\n",
    "    \"hoodie.datasource.hive_sync.table\": \"sales\",\n",
    "    \"hoodie.datasource.hive_sync.partition_fields\": \"country\",\n",
    "    \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\"\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# overwrite all records to s3 as parquet, partitioned by country, using hudi\n",
    "\n",
    "df_sales.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .options(**options_write_hudi) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%sh\n",
    "\n",
    "# preview hudi files in s3\n",
    "\n",
    "BASE_PATH=$(aws ssm get-parameter \\\n",
    "            --name \"/kafka_spark_demo/kafka_demo_bucket\" \\\n",
    "            --query \"Parameter.Value\" \\\n",
    "            --region \"us-east-1\" \\\n",
    "            --output text)\n",
    "\n",
    "aws s3api list-objects-v2 \\\n",
    "    --bucket $BASE_PATH --prefix \"hudi/\" \\\n",
    "    --query \"Contents[].Key\" --max-items 25"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# demonstrate hive integration (assumes hive installed on emr)\n",
    "\n",
    "spark.sql(\"SHOW databases\").show(25, truncate=False)\n",
    "spark.sql(\"USE `hudi`\")\n",
    "spark.sql(\"SHOW tables\").show(25, truncate=False)\n",
    "spark.sql(\"DESCRIBE sales\").show(25, truncate=False)\n",
    "spark.sql(\"MSCK REPAIR TABLE sales\")\n",
    "spark.sql(\"SELECT * FROM sales\").show(5, truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read data back from s3\n",
    "\n",
    "df_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_sales_snapshot.createOrReplaceTempView(\"hudi_sales_snapshot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%sql\n",
    "\n",
    "SELECT payment_id, payment_date, amount, city, district, country \n",
    "FROM hudi_sales_snapshot \n",
    "WHERE country=\"Japan\" \n",
    "ORDER BY payment_date \n",
    "LIMIT 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# update a row in dataframe with different payment amount and date vs. new message in kafka\n",
    "\n",
    "test_payment_id = df_sales.first()[\"payment_id\"]\n",
    "print(test_payment_id)\n",
    "\n",
    "df_update = df_sales \\\n",
    "    .filter(F.col(\"payment_id\") == test_payment_id) \\\n",
    "    .withColumn(\"payment_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"amount\", (F.lit(9.99)).cast(FloatType()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# show updated row\n",
    "\n",
    "df_update.show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# upsert modified record to S3 using hudi\n",
    "\n",
    "df_update.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .options(**options_write_hudi) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read updated data back from s3\n",
    "\n",
    "df_updated_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_updated_sales_snapshot.createOrReplaceTempView(\"df_updated_sales_snapshot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark.sql(f\"\"\"SELECT payment_id, customer_id, payment_date, amount, city, district, country \n",
    "                FROM df_updated_sales_snapshot \n",
    "                WHERE payment_id={test_payment_id}\"\"\") \\\n",
    "    .show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# delete the same record from s3 using hudi\n",
    "\n",
    "df_update.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.payload.class\",\n",
    "            \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\") \\\n",
    "    .options(**options_write_hudi) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# read updated data back from S3\n",
    "\n",
    "df_updated_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_updated_sales_snapshot.createOrReplaceTempView(\"df_updated_sales_snapshot\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this should return zero results since record was deleted\n",
    "\n",
    "spark.sql(f\"\"\"SELECT COUNT(payment_id) AS count\n",
    "                FROM df_updated_sales_snapshot\n",
    "                WHERE payment_id={test_payment_id}\"\"\") \\\n",
    "    .show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a dataframe with three new records\n",
    "\n",
    "# time.sleep(120)\n",
    "\n",
    "vals = [(int(99997), int(997), float(10.97), datetime.now(), \"Mumbai\", \"Maharashtra\", \"India\"),\n",
    "        (int(99998), int(998), float(2.98), datetime.now(), \"Paris\", \"Île-de-France\", \"France\"),\n",
    "        (int(99999), int(999), float(3.99), datetime.now(), \"Sunnyvale\", \"California\", \"United States\")]\n",
    "\n",
    "df_new_row = spark.createDataFrame(vals, schema)\n",
    "\n",
    "df_new_row.show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# write the new records to kafka topic\n",
    "\n",
    "write_to_kafka(df_new_row)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# batch read all messages back from kafka\n",
    "\n",
    "df_sales_new = read_from_kafka(kafka_topic, schema)\n",
    "\n",
    "df_sales_new.createOrReplaceTempView(\"df_sales_new_view\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM df_sales_new_view WHERE payment_id>=99990\") \\\n",
    "    .show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# write new rows of data to kafka topic\n",
    "\n",
    "df_sales_new.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .options(**options_write_hudi) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(base_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# incremental query with hudi to get a stream of records that have changed since a given commit timestamp\n",
    "\n",
    "begin_instant_time = \"2021-10-07 19:00:00:000\"\n",
    "\n",
    "options_read_hudi = {\n",
    "    \"hoodie.datasource.query.type\": \"incremental\",\n",
    "    \"hoodie.datasource.read.begin.instanttime\": begin_instant_time,\n",
    "}\n",
    "\n",
    "df_sales_new_hudi = spark.read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .options(**options_read_hudi) \\\n",
    "    .load(base_path)\n",
    "\n",
    "df_sales_new_hudi.createOrReplaceTempView(\"df_sales_new_hudi_view\")\n",
    "\n",
    "spark.sql(f\"SELECT * FROM df_sales_new_hudi_view WHERE payment_id>=99997\").show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# query hudi table in hive metastore for latest records vs. spark dataframe\n",
    "\n",
    "spark.sql(f\"\"\"SELECT payment_id, customer_id,\n",
    "                from_unixtime(payment_date/1000000) AS payment_date, \n",
    "                amount, city, district, country\n",
    "              FROM hudi.sales \n",
    "              WHERE payment_id>=99997\n",
    "              ORDER BY payment_id\"\"\") \\\n",
    "    .show(truncate=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}