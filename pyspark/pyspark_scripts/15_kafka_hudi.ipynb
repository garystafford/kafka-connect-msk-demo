{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "543236a8",
   "metadata": {},
   "source": [
    "## Demonstration: Apache Hudi with Kafka and S3\n",
    "\n",
    "__Purpose:__ Read messages from Kafka topic in JSON format and write to Amazon S3 as Parquet using Apache Hudi: Upserts and Delete  \n",
    "__Author:__  Gary A. Stafford  \n",
    "__Date:__ 2021-10-03  \n",
    "__References:__  \n",
    "- https://hudi.apache.org/docs/quick-start-guide/\n",
    "- https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hudi-work-with-dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990d80f",
   "metadata": {},
   "source": [
    "#### Run commands from master node\n",
    "\n",
    "```shell\n",
    "hdfs dfs -rm -r /mnt/tmp/\n",
    "# or\n",
    "hdfs dfs -chown -R livy /mnt/tmp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b660e",
   "metadata": {},
   "source": [
    "#### Run commands from master node\n",
    "\n",
    "```shell\n",
    "hdfs dfs -mkdir -p /apps/hudi/lib\n",
    "hdfs dfs -copyFromLocal /usr/lib/hudi/hudi-spark-bundle.jar /apps/hudi/lib/hudi-spark-bundle.jar\n",
    "hdfs dfs -copyFromLocal /usr/lib/spark/external/lib/spark-avro.jar /apps/hudi/lib/spark-avro.jar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7169fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars\":\n",
    "            \"hdfs:///apps/hudi/lib/hudi-spark-bundle.jar,hdfs:///apps/hudi/lib/spark-avro.jar\",\n",
    "        \"spark.serializer\":\n",
    "            \"org.apache.spark.serializer.KryoSerializer\",\n",
    "        \"spark.sql.hive.convertMetastoreParquet\":\n",
    "            \"false\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5457610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "import pyspark.sql.functions as F\n",
    "from ec2_metadata import ec2_metadata\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, FloatType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a90451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source of Kafka messages\n",
    "\n",
    "source_topic = \"pagila.sales.spark.streaming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bedbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws ssm parameter store values\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = ec2_metadata.region\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "\n",
    "params = {\n",
    "    \"kafka_servers\":\n",
    "        ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_servers\")\n",
    "        [\"Parameter\"][\"Value\"],\n",
    "    \"kafka_demo_bucket\":\n",
    "        ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_demo_bucket\")\n",
    "        [\"Parameter\"][\"Value\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a88c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch query Kafka topic\n",
    "\n",
    "options_read = {\n",
    "    \"kafka.bootstrap.servers\":\n",
    "        params[\"kafka_servers\"],\n",
    "    \"subscribe\":\n",
    "        source_topic,\n",
    "    \"startingOffsets\":\n",
    "        \"earliest\",\n",
    "    \"endingOffsets\":\n",
    "        \"latest\",\n",
    "    \"kafka.ssl.truststore.location\":\n",
    "        \"/tmp/kafka.client.truststore.jks\",\n",
    "    \"kafka.security.protocol\":\n",
    "        \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\":\n",
    "        \"AWS_MSK_IAM\",\n",
    "    \"kafka.sasl.jaas.config\":\n",
    "        \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "    \"kafka.sasl.client.callback.handler.class\":\n",
    "        \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\"\n",
    "}\n",
    "\n",
    "df_sales = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**options_read) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12cd0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dcd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert message payload from binary and deserialize JSON\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"amount\", FloatType(), False),\n",
    "    StructField(\"payment_date\", TimestampType(), False),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"district\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "])\n",
    "\n",
    "df_sales = df_sales \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .select(F.from_json(\"value\", schema=schema).alias(\"data\"), \"timestamp\") \\\n",
    "    .select(\"data.*\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da69bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02218e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all records to S3 as Parquet, partitioned by country, using Apache Hudi\n",
    "\n",
    "table_name = \"hudi.hudi_pagila_sales\"\n",
    "base_path = f\"s3://{params['kafka_demo_bucket']}/hudi/\"\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"payment_id\",\n",
    "    \"hoodie.datasource.write.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"country\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"timestamp\",\n",
    "    \"hoodie.upsert.shuffle.parallelism\": 2,\n",
    "    \"hoodie.insert.shuffle.parallelism\": 2,\n",
    "}\n",
    "\n",
    "df_sales.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b132d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data back from S3\n",
    "\n",
    "df_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_sales_snapshot.createOrReplaceTempView(\"hudi_sales_snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT payment_id, payment_date, amount, city, district, country \n",
    "FROM hudi_sales_snapshot \n",
    "WHERE country=\"Japan\" \n",
    "ORDER BY payment_date \n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e56ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update one record with different payment amount\n",
    "\n",
    "df_update = df_sales \\\n",
    "    .filter(F.col(\"payment_id\") == 16347) \\\n",
    "    .withColumn(\"payment_date\", F.current_timestamp()) \\\n",
    "    .withColumn(\"amount\", (F.lit(9.99)).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show updated record\n",
    "\n",
    "df_update.filter(F.col(\"payment_id\") == 16347).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsert record to S3 using Apache Hudi\n",
    "\n",
    "df_update.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb357599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read updated data back from S3\n",
    "\n",
    "df_updated_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_updated_sales_snapshot.createOrReplaceTempView(\"df_updated_sales_snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cea3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT payment_id, payment_date, amount, city, district, country \n",
    "FROM df_updated_sales_snapshot \n",
    "WHERE payment_id=16347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the same record from S3 using Apache Hudi\n",
    "\n",
    "df_update.write \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .option(\"hoodie.datasource.write.payload.class\", \n",
    "            \"org.apache.hudi.common.model.EmptyHoodieRecordPayload\") \\\n",
    "    .options(**hudi_options) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read updated data back from S3\n",
    "\n",
    "df_updated_sales_snapshot = spark \\\n",
    "    .read \\\n",
    "    .format(\"org.apache.hudi\") \\\n",
    "    .load(f\"{base_path}/*/*\")\n",
    "\n",
    "df_updated_sales_snapshot.createOrReplaceTempView(\"df_updated_sales_snapshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT payment_id, payment_date, amount, city, district, country \n",
    "FROM df_updated_sales_snapshot \n",
    "WHERE payment_id=16347"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac83acb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Last statement should return no results since record was deleted.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}