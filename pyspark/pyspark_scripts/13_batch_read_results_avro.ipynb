{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca74786",
   "metadata": {},
   "source": [
    "#### Batch query sales totals from Kafka in Avro format\n",
    "* __Author:__  Gary A. Stafford  \n",
    "* __Date:__ 2021-10-03  \n",
    "* __Post:__ [Stream Processing with Apache Spark, Kafka, Avro, and Apicurio Registry on Amazon EMR and Amazon MSK](https://itnext.io/stream-processing-with-apache-spark-kafka-avro-and-apicurio-registry-on-amazon-emr-and-amazon-13080defa3be)  \n",
    "* __Description:__  \n",
    "Notebook version of `13_batch_read_results_avro.py`. Script performs a batch query of all the Avro-format aggregated sales messages from the Kafka topic, `pagila.sales.summary.avro`, using schemas stored in Apicurio Registry. The script then summarizes the final sales results for each sliding 10-minute event-time window, by sales region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdeaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9701d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "from ec2_metadata import ec2_metadata\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50ea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema(artifact_id):\n",
    "    \"\"\"Get Avro schema from Apicurio Registry\"\"\"\n",
    "\n",
    "    response = requests.get(\n",
    "        f\"{params['schema_registry_url']}/apis/registry/v2/groups/default/artifacts/{artifact_id}\"\n",
    "    )\n",
    "    json_format_schema = response.content.decode(\"utf-8\")\n",
    "\n",
    "    return json_format_schema\n",
    "\n",
    "\n",
    "def get_parameters():\n",
    "    \"\"\"Load parameter values from AWS Systems Manager (SSM) Parameter Store\"\"\"\n",
    "\n",
    "    parameters = {\n",
    "        \"kafka_servers\":\n",
    "            ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_servers\")\n",
    "            [\"Parameter\"][\"Value\"],\n",
    "        \"kafka_demo_bucket\":\n",
    "            ssm_client.get_parameter(Name=\"/kafka_spark_demo/kafka_demo_bucket\")\n",
    "            [\"Parameter\"][\"Value\"],\n",
    "        \"schema_registry_url\":\n",
    "            ssm_client.get_parameter(\n",
    "                Name=\"/kafka_spark_demo/schema_registry_url_int\")[\"Parameter\"]\n",
    "            [\"Value\"],\n",
    "    }\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05939d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_topic = \"pagila.sales.summary.avro\"\n",
    "\n",
    "os.environ['AWS_DEFAULT_REGION'] = ec2_metadata.region\n",
    "ssm_client = boto3.client(\"ssm\")\n",
    "params = get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve schemas from registry\n",
    "\n",
    "sales_summary_key = get_schema(\"pagila.sales.summary.avro-key\")\n",
    "sales_summary_value = get_schema(\"pagila.sales.summary.avro-value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch query kafka topic\n",
    "\n",
    "options_read = {\n",
    "    \"kafka.bootstrap.servers\":\n",
    "        params[\"kafka_servers\"],\n",
    "    \"subscribe\":\n",
    "        source_topic,\n",
    "    \"startingOffsets\":\n",
    "        \"earliest\",\n",
    "    \"failOnDataLoss\":\n",
    "        \"false\",\n",
    "    \"kafka.ssl.truststore.location\":\n",
    "        \"/tmp/kafka.client.truststore.jks\",\n",
    "    \"kafka.security.protocol\":\n",
    "        \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\":\n",
    "        \"AWS_MSK_IAM\",\n",
    "    \"kafka.sasl.jaas.config\":\n",
    "        \"software.amazon.msk.auth.iam.IAMLoginModule required;\",\n",
    "    \"kafka.sasl.client.callback.handler.class\":\n",
    "        \"software.amazon.msk.auth.iam.IAMClientCallbackHandler\"\n",
    "}\n",
    "\n",
    "df_sales = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .options(**options_read) \\\n",
    "    .load() \\\n",
    "    .select(\"timestamp\",\n",
    "            from_avro(\"key\", sales_summary_key).alias(\"key\"),\n",
    "            from_avro(\"value\", sales_summary_value).alias(\"data\")) \\\n",
    "    .select(\"timestamp\", \"key\", \"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%display -n 10\n",
    "df_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw aggregated sales messages (kafka message value)\n",
    "\n",
    "df_sales \\\n",
    "    .drop(\"timestamp\", \"key\") \\\n",
    "    .show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396a2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of one region's aggregated sales values (Kafka messages)\n",
    "\n",
    "df_sales \\\n",
    "    .filter(F.col(\"region\") == \"Asia & Pacific\") \\\n",
    "    .select(F.date_format(\"timestamp\", format=\"yyyy-MM-dd HH:mm\").alias(\"timestamp\"),\n",
    "            F.col(\"region\").alias(\"sales_region\"),\n",
    "            F.format_number(\"sales\", 2).alias(\"sales\"),\n",
    "            F.format_number(\"orders\", 0).alias(\"orders\"),\n",
    "            F.from_unixtime(\"window_start\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_start\"),\n",
    "            F.from_unixtime(\"window_end\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_end\")) \\\n",
    "    .orderBy(F.col(\"window_end\").desc(), F.col(\"timestamp\").desc()) \\\n",
    "    .show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c2102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of single sales region's aggregated sales values during single 10-minute event window\n",
    "\n",
    "df_sales \\\n",
    "    .select(F.date_format(\"timestamp\", format=\"yyyy-MM-dd HH:mm\").alias(\"timestamp\"),\n",
    "            F.col(\"region\").alias(\"sales_region\"),\n",
    "            F.format_number(\"sales\", 2).alias(\"sales\"),\n",
    "            F.format_number(\"orders\", 0).alias(\"orders\"),\n",
    "            F.from_unixtime(\"window_start\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_start\"),\n",
    "            F.from_unixtime(\"window_end\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_end\")) \\\n",
    "    .filter(F.col(\"region\") == \"Asia & Pacific\") \\\n",
    "    .filter(F.col(\"window_start\") == \"2021-10-03 22:25\") \\\n",
    "    .orderBy(F.col(\"timestamp\").desc()) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c907e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get final sales for each region, by event-time window using Window.partitionBy and row_number().over(window)\n",
    "\n",
    "window = Window.partitionBy(\"region\",\n",
    "                            \"window_start\").orderBy(F.col(\"timestamp\").desc())\n",
    "\n",
    "df_sales_summary = df_sales \\\n",
    "    .withColumn(\"row\", F.row_number().over(window)) \\\n",
    "    .filter(F.col(\"row\") == 1).drop(\"row\") \\\n",
    "    .select(F.col(\"region\").alias(\"sales_region\"),\n",
    "            F.format_number(\"sales\", 2).alias(\"sales\"),\n",
    "            F.format_number(\"orders\", 0).alias(\"orders\"),\n",
    "            F.from_unixtime(\"window_start\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_start\"),\n",
    "            F.from_unixtime(\"window_end\", format=\"yyyy-MM-dd HH:mm\").alias(\"window_end\")) \\\n",
    "    .orderBy(F.col(\"window_start\").desc(),\n",
    "             F.regexp_replace(\"sales\", \",\", \"\").cast(\"float\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6ed08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_summary.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a163924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2562c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_sales_summary.show(36, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}