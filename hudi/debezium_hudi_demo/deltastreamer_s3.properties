include=base.properties

hoodie.datasource.hive_sync.database=moma
hoodie.datasource.hive_sync.table=artists

hoodie.datasource.write.recordkey.field=artist_id

hoodie.datasource.write.hive_style_partitioning=true
hoodie.datasource.hive_sync.assume_date_partitioning=false
hoodie.datasource.write.partitionpath.field=nationality
hoodie.datasource.hive_sync.partition_fields=nationality
hoodie.datasource.write.drop.partition.columns=true
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor

hoodie.deltastreamer.schemaprovider.source.schema.file=s3://<your_data_lake_bucket>/hudi/moma.public.artists-value.avsc
hoodie.deltastreamer.source.dfs.root=s3://<your_data_lake_bucket>/topics/moma.public.artists/partition=0/

# 1,024 * 1,024 * 128 = 134,217,728 (128 MB)
hoodie.parquet.small.file.limit=134217728

# https://dacort.dev/posts/updating-partition-values-with-apache-hudi/
# This is required if we want to ensure we upsert a record, even if the partition changes
hoodie.index.type=GLOBAL_BLOOM
# This is required to write the data into the new partition
# defaults to false in Apache Hudi 0.8.0 (EMR 6.4.0), true in 0.9.0)
hoodie.bloom.index.update.partition.path=true